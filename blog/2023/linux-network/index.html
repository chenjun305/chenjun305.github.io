<!DOCTYPE html> <html lang="cn"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>深入理解Linux网络 | Jun Chen</title> <meta name="author" content="Jun Chen"> <meta name="description" content="chenjun personal site and blog "> <meta name="keywords" content="blog"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://chenjun305.github.io/blog/2023/linux-network/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Jun Chen</span></a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">resume</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">深入理解Linux网络</h1> <p class="post-meta">March 11, 2023</p> <p class="post-tags"> <a href="/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a>   ·   <a href="/blog/tag/linux"> <i class="fas fa-hashtag fa-sm"></i> Linux</a>     ·   <a href="/blog/category/programming"> <i class="fas fa-tag fa-sm"></i> programming</a>   </p> </header> <article class="post-content"> <h2 id="内核是如何接收网络包的">内核是如何接收网络包的</h2> <p>当用户执行完recvfrom调用后，用户进程就通过系统调用进行到内核态工作了。如果接收队列没有数据，进程就进入睡眠状态被操作系统挂起。</p> <ol> <li>数据帧从外部网络到达网卡</li> <li>网卡把帧DMA到内存</li> <li>硬中断通知CPU</li> <li>CPU响应硬中断，简单处理后发出软中断</li> <li>ksoftirqd线程处理软中断，调用网卡驱动注册的poll函数开始收包</li> <li>帧被从RingBuffer上摘下来保存为一个SKB</li> <li>协议层开始处理网络帧，处理完后的数据data被放到Socket的接收队列中</li> <li>内核唤醒用户进程</li> </ol> <ul> <li>RingBuffer到底是什么, RingBuffer为什么会丢包？</li> </ul> <p>网卡在收到数据的时候以DMA的方式将包写到RingBuffer中。软中断收包的时候来这里把SKB取走，并申请新的SKB重新挂上去。</p> <p>RingBuffer的大小是可以设置的，长度可以通过ethtool工具查看。</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ ethtool -g ens3
Ring parameters for ens3:
Pre-set maximums:
RX:		256
RX Mini:	n/a
RX Jumbo:	n/a
TX:		256
Current hardware settings:
RX:		256
RX Mini:	n/a
RX Jumbo:	n/a
TX:		256
</code></pre></div></div> <p>如果内核处理不及时导致RingBuffer满了，那后面新来的数据包就会被丢弃，通过ethtool或ifconfig工具可以查看是否有RingBuffer溢出发生。</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ ethtool -S  ens3
NIC statistics:
     rx_queue_0_packets: 857
     rx_queue_0_bytes: 306826
     rx_queue_0_drops: 0
     rx_queue_0_xdp_packets: 0
     rx_queue_0_xdp_tx: 0
     rx_queue_0_xdp_redirects: 0
     rx_queue_0_xdp_drops: 0
     rx_queue_0_kicks: 1
     tx_queue_0_packets: 668
     tx_queue_0_bytes: 105070
     tx_queue_0_xdp_tx: 0
     tx_queue_0_xdp_tx_drops: 0
     tx_queue_0_kicks: 634
</code></pre></div></div> <p>修改RingBuffer大小, 不过改大之后会增加处理网络包的延时。另外一种解决思路更好，那就是让内核处理网络包的速度更快一些。</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ ethtool -G ens3 rx 4096 tx 4096
</code></pre></div></div> <ul> <li>ksoftirqd内核线程是干什么的？</li> </ul> <p>机器上有几个核，内核就会创建几个ksoftirqd线程出来。</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ ps -ef | grep ksoftirqd
root          13       2  0 20:50 ?        00:00:00 [ksoftirqd/0]
</code></pre></div></div> <p>ksoftirqd内核线程包含了所有的软中断处理逻辑。</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ cat /proc/softirqs
                    CPU0
          HI:          0
       TIMER:      21167
      NET_TX:          1
      NET_RX:       1908
       BLOCK:      11021
    IRQ_POLL:          0
     TASKLET:         74
       SCHED:          0
     HRTIMER:          0
         RCU:      22435
</code></pre></div></div> <ul> <li>为什么网卡开启多队列能提升网络性能</li> </ul> <p>现在主流网卡基本上都是支持多队列的，通过ethtool可以查看：</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ ethtool -l  ens3
Channel parameters for ens3:
Pre-set maximums:
RX:		n/a
TX:		n/a
Other:		n/a
Combined:	4
Current hardware settings:
RX:		n/a
TX:		n/a
Other:		n/a
Combined:	1
</code></pre></div></div> <p>通过sysfs文件系统可以看到真正生效的队列数。</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ ls /sys/class/net/ens3/queues/
rx-0  tx-0
</code></pre></div></div> <p>如果想加大队列数，ethtool也可以搞定</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ ethtool -L ens3 combined 2
</code></pre></div></div> <p>通过/proc/interrupts可以看到该队列对应的硬件中断号。再通过该中断号对应的smp_affinity可以查看到亲和的CPU核是哪一个？</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ cat /proc/interrupts
...

$ cat /proc/irq/&lt;中断号&gt;/smp_affinity
8
</code></pre></div></div> <p>这个亲和性是通过二进制中的比特位来标记的。例如8是二进制的1000， 第4位是1，代表是第4个CPU核心-CPU3</p> <p>每个队列都有独立的，不同的中断号。所以不同的队列可以向不同的CPU发起硬中断通知。且相应的软中断也是由这个核处理的。</p> <p>所以工作中，如果网络包的接收频率高而导致个别核si偏高，那么通过加大网卡队列数，并设置每个队列中断号上的smp_affinity, 将各个队列的硬中断打散到不同的CPU就行了。</p> <ul> <li>tcpdump是如何工作的</li> </ul> <p>tcpdump工作在设备层，是通过虚拟协议栈的形式工作的。将抓包函数以协议的形式挂到ptype_all上。</p> <p>当收包的时候，驱动在将包送到协议栈（ip_rcv， arp_rcv等）之前，将包先送到ptype_all抓包点。</p> <ul> <li>iptable/netfilter是在哪一层实现的？</li> </ul> <p>netfilter主要是在IP，ARP层实现的。如果配置过于复杂的规则，则会消耗过多CPU，加大网络延迟。</p> <ul> <li>tcpdump能否抓到被iptable封禁的包？</li> </ul> <p>tcpdump工作在设备层，而netfilter工作在IP/ARP层， 收包时，iptable封禁规则不影响tcpdump的抓包。 发包过程则相反，netfilter过滤后，tcpdump看不到被封禁的包。</p> <ul> <li>网络接收过程中的CPU开销如何查看</li> </ul> <p>在网络包的接收处理过程中，主要工作集中在硬中断和软中断上，二者的消耗都可以通过top命令查看。</p> <p>其中hi是cpu处理硬中断的开销，si是CPU处理软中断的开销，都是以百分比的形式来展示的。</p> <h2 id="内核是如何与用户进程协作的">内核是如何与用户进程协作的？</h2> <ul> <li>阻塞到底是怎么一回事？</li> </ul> <p>阻塞其实说的是进程因为等待某个事件而主动让出CPU挂起的操作。</p> <p>在网络IO中，当进程等待socket上的数据时，如果数据还没有到来，那就把当前进程状态从TASK_RUNNING 修改为 TASK_INTERRUPTIPLE, 然后主动让出CPU。 由调度器来调度下一个就绪的进程来执行。</p> <ul> <li>同步阻塞IO都需要哪些开销？ <ul> <li>从CPU开销角度看，一次同步阻塞IO将导致两次进程上下文切换开销。每一次切换大约花费3～5微妙。</li> <li>一个进程同时只能等待一条连接，如果有许多并发，则需要很多进程，每个进程都将占用大约几MB的内存。</li> </ul> </li> <li>多路复用epoll为什么就能提高网络性能？</li> </ul> <p>epoll高性能的最根本原因是极大程度地减少了无用的进程上下文切换，让进程更专注地处理网络请求。</p> <p>在内核的硬软中断上下文中，包从网卡接收过来进行处理，然后放到Socket的接收队列。再找到socket关联的epitem, 并把它添加到epoll对象的就绪链表中。</p> <p>在用户进程中，通过调用epoll_wait来查看就绪链表是否有事件到达，如果有，直接取走进行处理。处理完毕再次调用epoll_wait。在高并发的实践中，只要活儿足够多，epoll_wait根本不会让进程阻塞。</p> <p>直到epoll_wait里实在没活儿可干的时候才会让出CPU。这就是epoll高效的核心所在。</p> <ul> <li>为什么Redis的网络性能好？</li> </ul> <p>Redis在网络IO上表现非常突出，单进程的服务器在极限情况下可以达到10万的QPS。</p> <p>Redis的主要业务逻辑就是在本机内存上的数据结构的读写，单个请求处理起来很快。所以它把主服务端程序干脆做成了单线程的，这样省去了多线程之前协作的负担，也更大程度减少了线程切换。</p> <h2 id="内核是如何发送网络包的">内核是如何发送网络包的</h2> <ol> <li>用户进程send系统调用发送</li> <li>进入内核态，申请SKB，内存拷贝</li> <li>协议处理，传输层tcp头设置，滑动窗口管理 =》 网络层查找路由项，netfilter过滤，IP分片 =》 邻居子系统发送arp请求，获取目标MAC =》网络设备子系统，选择发送队列，skb入队</li> <li>进入驱动RingBuffer</li> <li>网卡实际发送</li> <li>网卡硬中断通知CPU发送完成</li> <li>触发软中断NET_RX_SOFTIRQ, 清理RingBuffer</li> </ol> <ul> <li>我们在监控内核发送数据消耗的CPU时，应该看sy还是si ?</li> </ul> <p>在网络包的发送过程中，用户进程（在内核态）完成了绝大部分工作，甚至连调用驱动的工作都干了。只当内核态进程被切走前才会发起软中断。 在发送过程中，绝大部分（90%）以上的开销都是在用户进程内核态消耗掉的。 只有一少部分情况才会触发软中断（NET_TX类型），由软中断ksoftirqd内核线程来发送。 所以，在监控网络IO对服务器造成的CPU开销的时候，不能仅看si, 而是应该把si, sy都考虑进来。</p> <ul> <li>在服务器上查看/proc/softirqs, 为什么NET_RX要比NET_TX大得多的多？ <ul> <li>原因1: 当数据发送完之后，触发的软中断是NET_RX_SOFTIRQ, 并不是NET_TX_SOFTIRQ.</li> <li>原因2: 收包时，都是要经过NET_RX软中断的，都走ksoftirqd内核线程。而发包时，绝大部分工作都是在用户进程内核态处理了，只有系统态配额用完才会发出NET_TX， 让软中断上。</li> </ul> </li> <li>发送网络数据的时候，都涉及哪些内存拷贝操作？ <ol> <li>将用户进程传递进来的buffer里的数据都拷贝到skb</li> <li>从传输层进入网络层的时候，进行浅拷贝，只拷贝skb描述符，所指向的数据复用。目的是网络对方没有回复ACK的时候，还可以重新发送，以实现TCP要求中的可靠传输。</li> <li>当网络层发现skb大于MTU时，进行分片，拷贝为多个小skb</li> </ol> </li> <li>零拷贝到底是怎么回事？为什么kafka的网络性能很突出？</li> </ul> <p>采用了sendfile系统调用来发送网络数据包，减少了内核态和用户态之间的频繁数据拷贝。</p> <p>而read + send系统调用发送文件过程如下：</p> <ol> <li>从硬盘DMA到内核态的page cache</li> <li>CPU拷贝page cache到用户内存</li> <li>cpu拷贝用户内存到socket发送缓冲区</li> <li>拷贝到RingBuffer</li> <li>DMA拷贝到网卡</li> </ol> <h2 id="深度理解本机网络io">深度理解本机网络IO</h2> <ul> <li>127.0.0.1本机网络IO需要经过网卡吗？</li> </ul> <p>不需要经过网卡，即使把网卡拔了，本机网络还是可以正常使用。</p> <ul> <li>数据包在内核中是什么走向，和外网发送相比流程上有什么差别？</li> </ul> <p>节约了驱动上的一些开销，发送数据不需要进RingBuffer的驱动队列，直接把skb传给接收协议栈（经过软中断）。但是 系统调用，协议栈（传输层，网络层等），设备子系统整个走了一遍。连“驱动”都走了（虽然回环设备是纯软件虚拟的）</p> <p>如果想在本机网络IO绕开协议栈的开销，可以使用eBPF, 使用eBPF的sockmap和sk redirect可以达到真正不走协议栈的目的。</p> <ul> <li>访问本机服务时，使用127.0.0.1能比使用其他IP更快吗？</li> </ul> <p>没有差别，都是走虚拟的环回设备lo， 这是因为内核在设置IP的时候，把所有的本机IP都初始化到local路由表里了。</p> <h2 id="深度理解tcp连接建立过程">深度理解TCP连接建立过程</h2> <ol> <li>服务端listen <ul> <li>申请并初始化接收队列，包括半连接队列和全连接队列</li> <li>全连接队列是1个链表，其最大长度min(listen时传入的backlog, net.core.somaxconn)</li> <li>半连接队列由于需要快速地查找，使用的是一个哈希表, 其最大长度是min(backlog, somaxconn, tcp_max_syn_backlog) + 1再向上取整到2的N次幂，但最小不能小于16</li> </ul> </li> <li>客户端connect <ul> <li>随机地从ip_local_port_range选择一个位置开始循环判断，选择可用的本地端口, 如果端口快用光了，内核大概率要循环多轮才能找到可用端口，这会导致connect系统调用的CPU开销上涨。如果端口查找失败，会报错“Cannot assign requested address”</li> <li>发出SYN握手请求</li> <li>启动重传定时器</li> </ul> </li> <li>服务端收到SYN握手请求 <ul> <li>发出SYC ACK</li> <li>进入半连接队列</li> <li>启动定时器</li> </ul> </li> <li>客户端收到SYN ACK <ul> <li>消除重传定时器</li> <li>设置为已连接</li> <li>发送ACK确认</li> </ul> </li> <li>服务端收到ACK <ul> <li>创建新sock</li> <li>从半连接队列删除</li> <li>加入全连接队列</li> </ul> </li> <li>服务端accept <ul> <li>从全连接队列取走socket</li> </ul> </li> </ol> <h3 id="握手异常总结">握手异常总结</h3> <ul> <li>如果端口不充足，处理方法有那么几个 <ul> <li>通过调整ip_local_port_range来尽量加大端口范围。</li> <li>尽量复用连接，使用长连接来削减频繁的握手处理</li> <li>有用但不太推荐的方法是开启tcp_tw_reuse和tcp_tw_recycle</li> </ul> </li> <li>半连接队列满，全连接队列满等导致丢包，应如何应对 <ul> <li>打开tcp_syncookies来防止过多请求打满半连接队列，包括SYN Flood攻击，来解决服务端因为半连接队列满而发生的丢包</li> <li>加大连接队列长度，可通过<code class="language-plaintext highlighter-rouge">ss -nlt</code>命令中输出的Send-Q来最终生效长度</li> <li>尽快调用accept, 应用程序应该尽快在握手成功后通过accept把新连接取走。</li> <li>尽早拒绝，例如将Redis，MySQL等服务器的内核参数tcp_abort_on_overflow设置为1，这是客户端会收到错误“connection reset by peer”</li> <li>用长连接代替短连接，减少过于频繁的三次握手</li> </ul> </li> </ul> <h2 id="一条tcp连接消耗多大内存">一条TCP连接消耗多大内存？</h2> <ul> <li>内核是如何管理内存的 <ol> <li>把所有内存条和CPU进行分组，组成node</li> <li>把每一个node划分成多个zone</li> <li>每个zone下都用伙伴系统来管理空闲页面</li> <li>提供slab分配器来管理各种内核对象， 每个slab缓存都是用来存储固定大小，甚至特定的一种内核对象。这样当一个对象释放后，另一个同类对象可以直接使用这块内存，几乎没有任何碎片，极大地提高了分配效率。</li> </ol> </li> <li>如何查看内核使用的内存信息 <ul> <li> <code class="language-plaintext highlighter-rouge">sudo cat /proc/slabinfo</code> 可以看到所有的kmem cache</li> <li> <code class="language-plaintext highlighter-rouge">sudo slabtop</code> 从大往小按照占用内存进行排列</li> </ul> </li> <li>服务器上一条ESTABLISH状态的空连接需要消耗多少内存？总共3.3KB左右 <ul> <li>struct socket_alloc, 大小约为0.62KB， slab缓存名是sock_inode_cache</li> <li>struct tcp_sock, 大小约为1.94KB， slab缓存名是tcp</li> <li>struct dentry, 大小约为0.19KB, slab缓存名是dentry</li> <li>struct file, 大小约为0.25KB, slab缓存名是flip</li> </ul> </li> <li>服务器上出现大量的TIME_WAIT, 内存开销会不会很大？ <ul> <li>一条TIME_WAIT状态的连接金占用0.4KB左右内存</li> <li>端口占用问题，可以考虑使用<code class="language-plaintext highlighter-rouge">tcp_max_tw_buckets</code>来限制TIME_WAIT连接总数，或者打开tcp_tw_recycle, tcp_tw_reuse来快速回收端口。</li> <li>使用长连接代替频繁的短连接</li> </ul> </li> </ul> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2023 Jun Chen. Last updated: December 13, 2023. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>